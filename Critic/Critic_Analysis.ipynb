{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "GrJl6XcFjS64"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from langdetect import detect\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch\n",
    "\n",
    "# Set a default device (GPU 0) for pipelines\n",
    "device_id = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_random_delay(delay=3, random_offset=0.5):\n",
    "    \"\"\"Pause execution for a short randomized delay.\"\"\"\n",
    "    time.sleep(delay + random.uniform(0, random_offset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "O_5_6jtgOtbS"
   },
   "outputs": [],
   "source": [
    "def extract_ign_review(soup):\n",
    "    \"\"\"\n",
    "    For IGN pages: extract review text from a container with data-cy=\"article-content\" or id=\"article-body\".\n",
    "    \"\"\"\n",
    "    container = soup.find(\"div\", {\"data-cy\": \"article-content\"})\n",
    "    if container is None:\n",
    "        container = soup.find(\"div\", {\"id\": \"article-body\"})\n",
    "    if container:\n",
    "        elements = container.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "        texts = [elem.get_text(separator=\" \", strip=True) for elem in elements if elem.get_text(strip=True)]\n",
    "        return \"\\n\".join(texts)\n",
    "    return None\n",
    "\n",
    "def extract_pcgamer_review(soup):\n",
    "    \"\"\"\n",
    "    For PCGamer pages: extract review text from a container with id or class \"article-body\".\n",
    "    \"\"\"\n",
    "    container = soup.find(\"div\", {\"id\": \"article-body\"})\n",
    "    if not container:\n",
    "        container = soup.find(\"div\", class_=\"article-body\")\n",
    "    if container:\n",
    "        elements = container.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "        texts = [elem.get_text(separator=\" \", strip=True) for elem in elements if elem.get_text(strip=True)]\n",
    "        return \"\\n\".join(texts)\n",
    "    return None\n",
    "\n",
    "def extract_eurogamer_review(soup):\n",
    "    \"\"\"\n",
    "    For Eurogamer pages: extract review text from a container with class \"article_body\" and data-component=\"article-content\",\n",
    "    or fallback to the first <section>.\n",
    "    \"\"\"\n",
    "    container = soup.find(\"div\", {\"data-component\": \"article-content\", \"class\": \"article_body\"})\n",
    "    if container is None:\n",
    "        container = soup.find(\"section\")\n",
    "    if container:\n",
    "        elements = container.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "        texts = [elem.get_text(separator=\" \", strip=True) for elem in elements if elem.get_text(strip=True)]\n",
    "        return \"\\n\".join(texts)\n",
    "    return None\n",
    "\n",
    "def extract_vg247_review(soup):\n",
    "    \"\"\"\n",
    "    For VG247 pages: extract review text from a container with class \"article_body\" and data-component=\"article-content\",\n",
    "    or fallback to a div with class \"article_body\".\n",
    "    \"\"\"\n",
    "    container = soup.find(\"div\", {\"data-component\": \"article-content\", \"class\": \"article_body\"})\n",
    "    if container is None:\n",
    "        container = soup.find(\"div\", class_=\"article_body\")\n",
    "    if container:\n",
    "        elements = container.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "        texts = [elem.get_text(separator=\" \", strip=True) for elem in elements if elem.get_text(strip=True)]\n",
    "        return \"\\n\".join(texts)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_critic_reviews(game_name, headers=None, websites=None):\n",
    "    \"\"\"\n",
    "    Scrapes critic reviews for a given game from four websites.\n",
    "    Returns a DataFrame with columns: 'website', 'review_text', 'url'.\n",
    "    \"\"\"\n",
    "    if headers is None:\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0\",\n",
    "            \"Accept-Language\": \"en-US,en;q=0.9\"\n",
    "        }\n",
    "    \n",
    "    # Define websites with URL pattern and corresponding extractor\n",
    "    websites = {\n",
    "        \"IGN\": {\n",
    "            \"url\": \"https://www.ign.com/articles/{slug}-review\",\n",
    "            \"extractor\": extract_ign_review\n",
    "        },\n",
    "        \"PCGamer\": {\n",
    "            \"url\": \"https://www.pcgamer.com/{slug}-review/\",\n",
    "            \"extractor\": extract_pcgamer_review\n",
    "        },\n",
    "        \"Eurogamer\": {\n",
    "            \"url\": \"https://www.eurogamer.net/articles/{slug}-review/\",\n",
    "            \"extractor\": extract_eurogamer_review\n",
    "        },\n",
    "        \"VG247\": {\n",
    "            \"url\": \"https://www.vg247.com/{slug}-review/\",\n",
    "            \"extractor\": extract_vg247_review\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    reviews_list = []\n",
    "    # Create a simple slug: lowercase, spaces to hyphens, remove apostrophes\n",
    "    slug = game_name.lower().replace(\" \", \"-\").replace(\"'\", \"\")\n",
    "    \n",
    "    for site_name, site_info in websites.items():\n",
    "        url = site_info['url'].format(slug=slug)\n",
    "        logging.info(f\"Scraping {site_name} from URL: {url}\")\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            if response.status_code != 200:\n",
    "                logging.warning(f\"Failed to retrieve review from {site_name}. HTTP Status: {response.status_code}\")\n",
    "                continue\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            review_text = site_info[\"extractor\"](soup)\n",
    "            if not review_text:\n",
    "                logging.warning(f\"No review text found on {site_name}.\")\n",
    "                continue\n",
    "            \n",
    "            reviews_list.append({\n",
    "                \"website\": site_name,\n",
    "                \"review_text\": review_text,\n",
    "                \"url\": url\n",
    "            })\n",
    "            \n",
    "            perform_random_delay(1, 0.3)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error while scraping {site_name}: {e}\")\n",
    "    \n",
    "    return pd.DataFrame(reviews_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_review_data(df, dropna=True, drop_duplicated=True, remove_spoiler=True, all_languages=False, selected_languages=['en']):\n",
    "    \"\"\"\n",
    "    Cleans the DataFrame of reviews:\n",
    "      - drops NA values and duplicates,\n",
    "      - removes standardized spoiler messages,\n",
    "      - and optionally filters by language.\n",
    "    \"\"\"\n",
    "    if dropna:\n",
    "        df = df.dropna()\n",
    "    if drop_duplicated:\n",
    "        df = df.drop_duplicates()\n",
    "    if remove_spoiler:\n",
    "        df = df[df['review_text'] != \"[SPOILER ALERT: This review contains spoilers.]\"]\n",
    "    \n",
    "    if not all_languages:\n",
    "        languages = []\n",
    "        for text in df['review_text']:\n",
    "            try:\n",
    "                languages.append(detect(text))\n",
    "            except Exception:\n",
    "                languages.append('unknown')\n",
    "        df['language'] = languages\n",
    "        df = df[df['language'].isin(selected_languages)]\n",
    "    \n",
    "    if len(selected_languages) <= 1 and 'language' in df.columns:\n",
    "        df.drop(columns=['language'], inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_aspects(text, aspects, classifier=None):\n",
    "    \"\"\"\n",
    "    Evaluates sentiment for each aspect keyword in the review text.\n",
    "    A positive sentiment adds the classifier's confidence score; negative subtracts it.\n",
    "    \"\"\"\n",
    "    if classifier is None:\n",
    "        # Use a (half-precision) ABSA model; loading in fp16 to reduce memory usage\n",
    "        model_name = \"yangheng/deberta-v3-base-absa-v1.1\"\n",
    "        tokenizer_absa = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "        model_absa = AutoModelForSequenceClassification.from_pretrained(model_name, torch_dtype=torch.float16).to(\"cuda\")\n",
    "        classifier = pipeline(\"text-classification\", model=model_absa, tokenizer=tokenizer_absa, device=device_id)\n",
    "    \n",
    "    aspect_scores = {}\n",
    "    for aspect in aspects:\n",
    "        try:\n",
    "            result = classifier(text, text_pair=aspect)[0]\n",
    "            score = result['score'] if result['label'] == 'Positive' else -result['score']\n",
    "            aspect_scores[aspect] = score\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"ABSA error for aspect '{aspect}': {e}\")\n",
    "            aspect_scores[aspect] = 0.0\n",
    "    return aspect_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text, summarizer=None):\n",
    "    \"\"\"\n",
    "    Summarizes the given text using a pretrained summarization model.\n",
    "    If the text is too long for the model (i.e. more tokens than the maximum input length),\n",
    "    the text is split into chunks, each chunk is summarized, and the summaries are combined.\n",
    "    \"\"\"\n",
    "    if summarizer is None:\n",
    "        summarizer = pipeline(\"summarization\", model=\"t5-small\", tokenizer=\"t5-small\", device=device_id)\n",
    "    try:\n",
    "        tokenizer = summarizer.tokenizer\n",
    "        max_input_length = tokenizer.model_max_length  # usually 512 tokens for T5-small\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=False)\n",
    "        input_length = inputs.input_ids.shape[1]\n",
    "        \n",
    "        if input_length <= max_input_length:\n",
    "            summary = summarizer(text, max_length=150, min_length=40, do_sample=False)\n",
    "            return summary[0]['summary_text']\n",
    "        else:\n",
    "            # Split text into chunks by sentences\n",
    "            sentences = re.split(r'(?<=[.?!])\\s+', text)\n",
    "            chunks = []\n",
    "            current_chunk = \"\"\n",
    "            for sentence in sentences:\n",
    "                test_chunk = current_chunk + \" \" + sentence if current_chunk else sentence\n",
    "                test_tokens = tokenizer.encode(test_chunk, add_special_tokens=True)\n",
    "                if len(test_tokens) <= max_input_length:\n",
    "                    current_chunk = test_chunk\n",
    "                else:\n",
    "                    if current_chunk:\n",
    "                        chunks.append(current_chunk)\n",
    "                    current_chunk = sentence\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk)\n",
    "            \n",
    "            # Summarize each chunk and combine the summaries\n",
    "            summaries = []\n",
    "            for chunk in chunks:\n",
    "                summ = summarizer(chunk, max_length=150, min_length=40, do_sample=False)\n",
    "                summaries.append(summ[0]['summary_text'])\n",
    "            combined_summary = \" \".join(summaries)\n",
    "            \n",
    "            # Re-summarize if necessary\n",
    "            inputs_combined = tokenizer(combined_summary, return_tensors=\"pt\", truncation=False)\n",
    "            if inputs_combined.input_ids.shape[1] > max_input_length:\n",
    "                final_summary = summarizer(combined_summary, max_length=150, min_length=40, do_sample=False)\n",
    "                return final_summary[0]['summary_text']\n",
    "            else:\n",
    "                return combined_summary\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Summarization error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def comment_analysis_with_summary(game_name, aspects, headers=None):\n",
    "    \"\"\"\n",
    "    Performs the following steps:\n",
    "      1. Loads the ABSA model and a T5-small summarizer.\n",
    "      2. Scrapes critic reviews for the game.\n",
    "      3. Cleans the review data.\n",
    "      4. For each review, computes aspect sentiment scores.\n",
    "      5. Combines all review texts into one aggregated text and summarizes it.\n",
    "      6. Returns overall aspect scores (averaged over reviews), the combined summary, and the DataFrame.\n",
    "    \"\"\"\n",
    "    logging.info(\"Loading ABSA model for aspect analysis...\")\n",
    "    model_name_absa = \"yangheng/deberta-v3-base-absa-v1.1\"\n",
    "    tokenizer_absa = AutoTokenizer.from_pretrained(model_name_absa, use_fast=False)\n",
    "    model_absa = AutoModelForSequenceClassification.from_pretrained(model_name_absa, torch_dtype=torch.float16).to(\"cuda\")\n",
    "    classifier = pipeline(\"text-classification\", model=model_absa, tokenizer=tokenizer_absa, device=device_id)\n",
    "    logging.info(\"ABSA model loaded on GPU.\")\n",
    "\n",
    "    logging.info(\"Initializing summarization pipeline (T5-small)...\")\n",
    "    summarizer = pipeline(\"summarization\", model=\"t5-small\", tokenizer=\"t5-small\", device=device_id)\n",
    "    logging.info(\"Summarization model loaded on GPU.\")\n",
    "\n",
    "    logging.info(\"Scraping critic reviews...\")\n",
    "    df_reviews = scrape_critic_reviews(game_name, headers=headers)\n",
    "    if df_reviews.empty:\n",
    "        logging.error(\"No reviews scraped. Exiting analysis.\")\n",
    "        return {}, \"\", df_reviews\n",
    "\n",
    "    logging.info(\"Cleaning review data...\")\n",
    "    df_reviews = clean_review_data(df_reviews)\n",
    "\n",
    "    logging.info(\"Performing aspect-based sentiment analysis...\")\n",
    "    # Compute aspect scores for each review and average them for overall scores\n",
    "    scores_df = df_reviews['review_text'].apply(lambda x: pd.Series(find_aspects(x, aspects, classifier=classifier)))\n",
    "    df_reviews = pd.concat([df_reviews, scores_df], axis=1)\n",
    "    logging.info(\"Aspect analysis complete!\")\n",
    "\n",
    "    # Combine all review texts into one aggregated text\n",
    "    combined_text = \"\\n\\n\".join(df_reviews['review_text'].tolist())\n",
    "    logging.info(\"Generating overall summary from combined reviews...\")\n",
    "    overall_summary = summarize_text(combined_text, summarizer=summarizer)\n",
    "    logging.info(\"Overall summarization complete!\")\n",
    "\n",
    "    overall_aspect_scores = scores_df.mean().to_dict()  # Averaging scores for better interpretability\n",
    "    \n",
    "    return overall_aspect_scores, overall_summary, df_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Example usage: analyze \"baldurs gate 3\" reviews for several aspects\n",
    "    game_name = \"red dead redemption 2\"\n",
    "    aspects = ['graphics', 'gameplay', 'story', 'performance']\n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                      \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                      \"Chrome/90.0.4430.85 Safari/537.36\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\"\n",
    "    }\n",
    "    \n",
    "    overall_aspect_scores, overall_summary, df_reviews = comment_analysis_with_summary(game_name, aspects, headers=headers)\n",
    "    \n",
    "    # Print the overall summary based on all scraped reviews\n",
    "    print(\"Overall Summary of Scraped Reviews:\")\n",
    "    print(overall_summary)\n",
    "    \n",
    "    # Print overall averaged aspect scores\n",
    "    print(\"\\nOverall Aspect Scores (averaged over reviews):\")\n",
    "    for aspect, score in overall_aspect_scores.items():\n",
    "        print(f\"{aspect}: {score:.3f}\")\n",
    "    \n",
    "    # Plot overall aspect scores using an improved horizontal bar chart with a diverging colormap\n",
    "    import matplotlib.cm as cm\n",
    "    labels = list(overall_aspect_scores.keys())\n",
    "    scores = [overall_aspect_scores[aspect] for aspect in labels]\n",
    "    \n",
    "    # Sorting for a cleaner horizontal bar chart:\n",
    "    sorted_data = sorted(zip(labels, scores), key=lambda x: x[1])\n",
    "    sorted_labels, sorted_scores = zip(*sorted_data)\n",
    "    \n",
    "    # Define colormap: if scores span negative to positive, use a diverging colormap.\n",
    "    min_score = min(sorted_scores)\n",
    "    max_score = max(sorted_scores)\n",
    "    if max_score - min_score == 0:\n",
    "        colors = ['skyblue'] * len(sorted_scores)\n",
    "    else:\n",
    "        colors = [cm.coolwarm((s - min_score) / (max_score - min_score)) for s in sorted_scores]\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    bars = plt.barh(sorted_labels, sorted_scores, color=colors)\n",
    "    plt.xlabel('Average Sentiment Score')\n",
    "    plt.title('Overall Aspect Sentiment Scores')\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Annotate bars with their values\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        plt.text(width, bar.get_y() + bar.get_height()/2,\n",
    "                 f'{width:.2f}', ha='left', va='center', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
